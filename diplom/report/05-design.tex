\chapter{Конструкторский раздел}

\section{Входные данные}
На вход методу подается атрибуты страницы, к которой происходит обращение, и атрибуты всех страниц, которые уже находятся в буфере.

Для сохранения истории обращений к кэш буферу в функцию ReadBufferExtended был добавлен вызов функции печати в лог файл атрибутов страницы, к которой идет обращение.
Эта функция вызывается, каждый раз, когда необходимо прочитать страницу из буфера.

Для каждой страницы извлекается следующий набор атрибутов:
\begin{itemize}
	\item идентификатор отношения;
	\item номер страницы в файле;
	\item наличие индекса;
	\item позиция в буфере.
\end{itemize}

Если в момент обращения к странице она не находится в буфере, то позиция задается размером буфера.

\section{Проектирование метода}
IDEF-0 диаграмма метода замещения страниц уровня A1 приведена на рисунке \ref{img:idef0A1}.
\includeimage
{idef0A1} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{\textwidth} % Ширина рисунка
{IDEF-0 диаграмма уровня A1} % Подпись рисунка

\textbf{Кодировщик запроса обращения к странице} отвечает за скрытое представление атрибутов страницы, к которой происходит очередное обращение.
Схема кодировщика запроса обращения к странице изображена на рисунке \ref{img:page_acc}.
\includeimage
{page_acc} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.6\textwidth} % Ширина рисунка
{Схема кодировщика запроса обращения к странице} % Подпись рисунка

На вход кодировщика поступают $n$ атрибутов страницы.
Каждый атрибут может иметь $m_i$ возможных значений, где $i$ -- индекс атрибута.
Каждый атрибут представляется в виде вектора $a^{(i)}$ размерности $m_i$.
Для категориальных данных используется техника однозначного кодирования, а для числовых -- применяется хэш функция и к полученному результату применяется техника однозначного кодирования.
$W_1^{(i)}$ -- матрица обучаемых весов для скрытого представления i-го атрибута.
Вектор $z$ -- выходной вектор из сети.
$W_1$ -- матрица обучаемых весов, при помощи который получается результирующий вектор из скрытых представлений атрибутов сети.
В качестве функции активации на последнем слое используется функция Relu.
$d_f$ и $d_z$ являются настраиваемыми параметрами, которые отвечают за число нейронов, отвечающий за скрытое представление каждого атрибута, и число нейронов на выходном слое соответственно.

Работу сети можно описать с помощью выражений
\ref{formula:page_enc_1} - \ref{formula:page_enc_3}:
\begin{equation}\label{formula:page_enc_1}
	f^{(i)} = a^{(i)}W_1^{(i)} i \in \{1;n\},
\end{equation}

\begin{equation}\label{formula:page_enc_2}
	f = [f^{(1)}, f^{(2)}, ..., f^{(n)}],
\end{equation}

\begin{equation}\label{formula:page_enc_3}
	z = ReLU(W_1f^T + l_1),
\end{equation}
где $f$ является конкатенацией векторов скрытых состояний атрибутов страницы, а $l_1$ -- обучаемым вектором.

\textbf{Кодировщик страниц в буфере} нужен для скрытого представления каждой страницы в буфере.
Схема кодировщика представлена на рисунке \ref{img:buf_page_enc}
\includeimage
{buf_page_enc} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Схема кодировщика страниц в буфере} % Подпись рисунка

На вход кодировщика поступают $M$ страниц из буфера.
Каждая страница представляется в виде $n$ атрибутов.
Процесс обработки атрибутов для каждой страницы такой же, как и в кодировщике запроса обращения к странице.
Для каждой i-ой страницы в буфере вычисляется вектор $b_i$.
$d_b$ является настраиваемым параметром, который отвечает за размерность векторов $b_i$.

Для получения скрытого представления каждого атрибута и для вычисления закодированного представления страницы используется одни и те же матрицы весов $W_2^{(i)}$ и $W_2$ для всех страниц в буфере.
За счет этого матрицы весов не привязаны к конкретной позиции страницы в буфере и истории страниц на этой позиции.
При обратном распространении ошибки влияние веса из матрицы $W_2$ будет учитываться для всех векторов $b_i$.

Обозначим результат работы сумматора нейрона на выходном слое как $s_{ij}$.
Индексация в матрице $s$ совпадает с матрицей $b$.
Тогда для вычисления ошибки по весу $w_{ij}$ из матрицы $W_2$ на ребре, которое соединяет j-ый нейрон из второго слоя и i-ый нейрон из выходного слоя, используется выражение \ref{formula:buf_page_enc_err}:
\begin{equation}\label{formula:buf_page_enc_err}
	\frac{\delta E}{\delta w_{ij}} = \sum\limits_{k=1}^{M}\frac{\delta E}{\delta b_{ki}}\frac{\delta b_{ki}}{\delta s_{ki}} \frac{\delta s_{ki}}{\delta w_ij},
\end{equation}
где $E$ -- функция ошибки, $\frac{\delta E}{\delta b_{ki}}$ -- ошибка полученная со следующего слоя.

Функционирование кодировщика определяется выражениями
\ref{formula:buf_page_enc_1} - \ref{formula:buf_page_enc_3}:
\begin{equation}\label{formula:buf_page_enc_1}
	f^{(j,i)} = a^{(j,i)}W_2^{(i)} j \in \{1;M\} i \in \{1;n\},
\end{equation}

\begin{equation}\label{formula:buf_page_enc_2}
	f^{(j)} = [f^{(j,1)}, f^{(j,2)}, ..., f^{(j,n)}],
\end{equation}

\begin{equation}\label{formula:buf_page_enc_3}
	b_j = ReLU(W_2f^{(j)T} + l_2),
\end{equation}
где $f^{(j)}$ -- конкатенация скрытых представлений атрибутов для j-ой страницы в буфере, $b_j$ -- скрытое представление этой страницы, $l_2$ -- вектор обучаемых весов.

\textbf{Кодировщик истории обращений.} Для обновления истории обращений используется сеть LSTM.
На вход сети поступают результат работы кодировщика обращения к странице, предыдущий результат кодировщика истории обращений и предыдущее состояние ячейки.

Функционирование кодировщика определяется выражениями
\ref{formula:lstm_enc_1} - \ref{formula:lstm_enc_6}:
\begin{equation}\label{formula:lstm_enc_1}
	f_t = \sigma(W_f[h_{t-1}, z_t] + b_f),
\end{equation}

\begin{equation}\label{formula:lstm_enc_2}
	i_t = \sigma(W_i[h_{t-1}, z_t] + b_i),
\end{equation}

\begin{equation}\label{formula:lstm_enc_3}
	\hat{C_t} = \tanh(W_C[h_{t-1}, z_t] + b_C),
\end{equation}

\begin{equation}\label{formula:lstm_enc_4}
	C_t = f_t * C_{t-1} + i_t*\hat{C_t},
\end{equation}

\begin{equation}\label{formula:lstm_enc_5}
	o_t = \sigma(W_o[h_{t-1}, z_t] + b_o),
\end{equation}

\begin{equation}\label{formula:lstm_enc_6}
	h_t = o_t * \tanh(C_t),
\end{equation}

где $[h_{t-1}, z_t]$ -- конкатенация результата работы предыдущего слоя кодировщика истории и скрытого состояния, полученного из кодировщика обращения к странице, $W_f$ и $b_f$ -- матрица и вектор обучаемых весов, $f_t$ -- результат работы фильтра забывания, $i_t$ определяет, какие значения будут сохранены в ячейке, $\hat{C_t}$ -- новые значения кандидатов на попадание в ячейку, $W_i$, $W_C$, $b_i$, $b_c$ -- матрицы и вектора обучаемых весов, $C_t$ -- новое состояние ячейки, $C_{t-1}$ -- состояние ячейки на прошлом шаге, $h_t$ -- результат работы текущего слоя, $C_t$ -- состояние ячейки, $W_o$ и $b_o$ -- матрица и вектор обучаемых весов.
Вектора $h_t$ и $C_t$ имеют размерность $d_h$, где $d_h$ -- настраиваемый параметр.

\textbf{Модуль выбора страниц для замещения.}
На вход модуля поступают результаты работы кодировщика страниц в буфере и кодировщика истории обращений.
Для выбора страницы, которая будет удалена из буфера используется указательная нейронная сеть с механизмом внимания \cite{vinyals2015pointer}.

Нейронные сети с механизмом внимания -- это архитектуры, которые позволяют моделям динамически фокусироваться на наиболее релевантных частях входных данных при обработке информации.
Этот подход нашел применения в областях обработки естественного языка, компьютерного зрения и других задач, где важно учитывать контекст и зависимости между элементами последовательности.
В модуле выбора страниц для замещения контекстом является результат работы кодировщика истории, а элементами последовательности -- результаты работы кодировщика страниц в буфере.

Указательные сети -- архитектура сетей с механизмом внимания, предназначенная для решения задач, где выходные элементы представляют собой позиции в входной последовательности.
В указательных сетях механизм внимания используется как указатель на один из элементов входной последовательности, а не для создания контекстного вектора, как в классических моделях с механизмом внимания.

Схема модуля выбора страниц для замещения представлена на рисунке \ref{img:decision_maker}
\includeimage
{decision_maker} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Схема модуля выбора страниц для замещения} % Подпись рисунка

$W_4$ -- матрица обучаемых весов, которая используется для преобразования вектора $h$, полученного из кодировщика истории в вектор контекста размерности $d_v$. $d_v$ является настраиваемым параметром модели.

$W_3$ -- матрица обучаемых весов, которая используется для преобразования закодированного состояния очередной страницы в буфере в вектор размерности $d_v$, который будет использован в функции внимания.

$v$ -- вектор обучаемых весов, который который используется при вычислении функции внимания.

Функционирование модуля выбора страниц для замещения определяется выражениями \ref{formula:evict_1} - \ref{formula:evict_3}:
\begin{equation}\label{formula:evict_1}
	u_i = v * \tanh(W_3 b_i^T + W_4 h^T), i \in \{1; M\},
\end{equation}

\begin{equation}\label{formula:evict_2}
	p_i = SoftMax(u_i),
\end{equation}

\begin{equation}\label{formula:evict_3}
	r = \arg\max_{i} p_i,
\end{equation}
где $M$ -- число страниц в буфере, $u_i$ -- результат функции внимания для i-ой страницы в буфере, $\arg\max_{i} p_i$ -- функция, которая возвращает индекс максимального элемента в последовательности, $r$ -- результат работы спроектированного метода замещения страниц.

Схемы алгоритмов обучения нейронных сетей и прохождения одной эпохи приведены на рисунках \ref{img:training} и \ref{img:training_epoch} соответственно.
\includeimage
{training} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.6\textwidth} % Ширина рисунка
{Схема алгоритма обучения нейронной сети} % Подпись рисунка

\includeimage
{training_epoch} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.6\textwidth} % Ширина рисунка
{Схема алгоритма прохождения одной эпохи} % Подпись рисунка

\section{Структура программного обеспечения}

Программное обеспечение состоит из пяти модулей:
\begin{itemize}
	\item модуль получения обучающей выборки;
	\item кодировщик запросов обращения к страницам;
	\item кодировщик истории запросов обращения к страницам;
	\item кодировщик страниц в буфере;
	\item модуль выбора страниц для замещения.
\end{itemize}

Структурная схема взаимодействия модулей разрабатываемого программного обеспечения представлена на рисунке \ref{img:modules}.

\includeimage
{modules} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Структура программного обеспечения} % Подпись рисунка

Модуль получения обучающей выборки нужен для обработки лог файла и создания обучающей и тестовой выборок.
Остальные модули предназначены для обучения и взаимодействия с уже обученными моделями.

\section{Набор обучающих данных}
Для имитации нагрузки на СУБД и получения истории обращений к страницам был использован тестовый сценарий TPC-C \cite{leutenegger1993modeling}.

TPC-C -- это стандартный тест для оценки производительности систем, обрабатывающих транзакции в режиме реального времени. 
Он имитирует работу оптового поставщика с распределённой сетью складов и предназначен для тестирования СУБД на реалистичных сценариях высокой нагрузки.

Для обработки нагрузки создается база данных, состоящая из следующих таблиц:
\begin{itemize}
	\item Warehouse (склады);
	\item District (регионы складов);
	\item Customer (клиенты);
	\item Order (заказы);
	\item Order-Line (позиции заказов);
	\item Item (товары);
	\item Stock (запасы на складах);
	\item History (история платежей).
\end{itemize}

TPC-C включает 5 типов транзакций, имитирующих реальные бизнес-операции:
\begin{enumerate}
	\item NewOrder (45\%): создание нового заказа (вставка данных в таблицы Order, Order-Line, обновление Stock).
	\item Payment (43\%): Обработка платежа (обновление Customer, Warehouse, District, вставка в History).
	\item Delivery (4\%): Доставка заказа (удаление из Order, обновление Customer).
	\item OrderStatus (4\%): Проверка статуса заказа (выборка из Order, Order-Line, Customer).
	\item StockLevel (4\%): Проверка уровня запасов (выборка из Stock).
\end{enumerate}

С помощью тестовой нагрузки было получено 6 554 959 обращений к страницам.
Выборка была поделена на тренировочную и тестовую в отношении семь к трем.

Корреляция между атрибутами страниц, посчитанная на всей выборке, представлена на рисунке \ref{img:correlation}.

\includeimage
{correlation} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Корреляция между атрибутами страниц} % Подпись рисунка

Корреляция вычислялась по формуле Пирсона \ref{formula:pirson}:
\begin{equation}\label{formula:pirson}
	c = \frac{\sum\limits_{i=1}^{n} (x_i - \overline{x}) (y_i - \overline{y})}{\sqrt{\sum\limits_{i=1}^{n}(x_i - \overline{x})^2} \sqrt{\sum\limits_{i=1}^{n}(y_i - \overline{y})^2}},
\end{equation}
где $n$ -- число элементов в последовательности, $x_i$ и $y_i$ -- элементы последовательностей, между которыми считается корреляция, $\overline{x}$ и $\overline{y}$ -- средние значения элементов последовательностей.

Количество замещений страницы оптимальным алгоритмом в зависимости от индекса страницы в буфере для тренировочной и тестовой выборок приведено на рисунках \ref{img:evict_freq_train} и \ref{img:evict_freq_test} соответственно.

\includeimage
{evict_freq_train} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Количество замещений страницы по индексу на тренировочной выборке} % Подпись рисунка

\includeimage
{evict_freq_test} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Количество замещений страницы по индексу на тестовой выборке} % Подпись рисунка


\section{Вывод}

В данном разделе были определены ограничения, которые накладываются на входные данные, и требования, которые предъявляются к разрабатываемому программному обеспечению.

Была детализирована IDEF-0 диаграмма уровня А0, описанная в разделе формализованной постановки задачи, а также было проведено разбиение программного обеспечения на модули.
Задача выбора страниц для замещения из буфера была разбита на четыре подзадачи: кодирование запроса обращения к странице, обновление истории обращений, кодирование страниц в буфере, выбор страниц для замещения на основе контекста.

Для решения каждой подзадачи была спроектирована архитектура нейронной сети.
Для обеспечения временных зависимостей при обновлении истории была выбрана сеть LSTM.
Для выбора страницы для замещения была спроектирована указательная сеть с механизмом внимания, которая выбирает одну из страниц в буфере для замещения.
Была определена схема алгоритма обучения составленных нейронных сетей.

Для генерации реалистичной нагрузки использован тестовый сценарий TPC-C, что позволило получить шесть с половинной миллионов обращений к страницам.
Распределение замещений страниц на тренировочной и тестовой выборках подтвердило сбалансированность данных.