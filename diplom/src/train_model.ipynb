{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import seaborn as sns\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass, fields, asdict\n",
    "from model.model import PageAccModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Page:\n",
    "    buffer: int\n",
    "    # is_local_temp: int\n",
    "    fork_num: int\n",
    "    block_num: int\n",
    "    # mode: int\n",
    "    relam: int\n",
    "    relfilenode: int\n",
    "    relhasindex: int\n",
    "    # relpersistence: int\n",
    "    relkind: int\n",
    "    relnatts: int\n",
    "    relfrozenxid: int\n",
    "    relminmxid: int\n",
    "    hit: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data/acc_logfile2\", \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "pattern = r\"buffer={(\\d+)} is_local_temp={(\\w+)} fork_num={(\\w+)} block_num={(\\d+)} mode={(\\w+)} strategy={} relam={(\\d+)} relfilenode={(\\d+)} relhasindex={(\\w+)} relpersistence={(\\w+)} relkind={(\\w+)} relnatts={(\\d+)} relfrozenxid={(\\d+)} relminmxid={(\\d+)} hit={(\\w+)}\"\n",
    "matches = re.findall(pattern, data)\n",
    "\n",
    "pages = []\n",
    "\n",
    "for match in matches:\n",
    "    buffer = int(match[0])\n",
    "    is_local_temp = 1 if match[1] == \"true\" else 0\n",
    "    fork_num = [\"MAIN_FORKNUM\", \"FSM_FORKNUM\", \"VISIBILITYMAP_FORKNUM\", \"INIT_FORKNUM\"].index(match[2])\n",
    "    block_num = int(match[3])\n",
    "    mode = [\"RBM_NORMAL\", \"RBM_ZERO_AND_LOCK\", \"RBM_ZERO_AND_CLEANUP_LOCK\", \"RBM_ZERO_ON_ERROR\", \"RBM_NORMAL_NO_LOG\"].index(match[4])\n",
    "    relam = int(match[5])\n",
    "    relfilenode = int(match[6])\n",
    "    relhasindex = 1 if match[7] == \"true\" else 0\n",
    "    relpersistence = [\"p\", \"u\", \"t\"].index(match[8])\n",
    "    relkind = [\"r\", \"i\", \"S\", \"t\", \"v\", \"m\", \"c\", \"f\", \"p\", \"I\"].index(match[9])\n",
    "    relnatts = int(match[10])\n",
    "    relfrozenxid = int(match[11])\n",
    "    relminmxid = int(match[12])\n",
    "    hit = 1 if match[13] == \"true\" else 0\n",
    "\n",
    "    page = Page(\n",
    "        buffer,\n",
    "        # is_local_temp, \n",
    "        fork_num,\n",
    "        block_num,\n",
    "        #mode,\n",
    "        relam,\n",
    "        relfilenode,\n",
    "        relhasindex,\n",
    "        # relpersistence,\n",
    "        relkind,\n",
    "        relnatts,\n",
    "        relfrozenxid,\n",
    "        relminmxid,\n",
    "        hit,\n",
    "    )\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data/acc_logfile2_pages\", \"w\") as f:\n",
    "    for page in pages:\n",
    "        f.write(f\"{page.buffer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024067\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_results = []\n",
    "with open(\"train_data/acc_logfile2_victims\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        victims_count = int(line)\n",
    "        victims_rates = []\n",
    "        for _ in range(victims_count):\n",
    "            victim_rate = f.readline().strip().split()\n",
    "            victims_rates.append([int(victim_rate[0]), int(victim_rate[1])])\n",
    "\n",
    "        optimal_results.append(victims_rates)\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024067"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(optimal_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = len(pages)\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024067"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6318912727389907"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([page.hit for page in pages[:TRAIN_SIZE]]) / TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6844718167854251"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find_optimal_hit_rate([page.buffer for page in pages[:TRAIN_SIZE]], BUFFER_SIZE)\n",
    "sum([1 if len(victims) == 0 else 0 for victims in optimal_results[:TRAIN_SIZE]]) / TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot([victims[0][0] for victims in optimal_results[:TRAIN_SIZE] if len(victims) != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='None'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAGwCAYAAABFFQqPAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKXVJREFUeJzt3XtQ1fed//EXoBy8nUNQgTDiZcckSr1QQfHUJLM2rCcJycQVt5qykSjR0YKNnsQLrUVrOzHVMV6KkU3dBjsbJ+ju6CZSMQ6umlXiBcNWbbRJ1x3I0gO6yjnKRFDg90d/fIejJIAGj/B5PmbOTPh+3+d73jJDfM7hnGNQU1NTkwAAAAwUHOgFAAAAAoUQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxegR6gQdZY2OjKisr1a9fPwUFBQV6HQAA0A5NTU26du2aYmJiFBz8zc/5EELfoLKyUrGxsYFeAwAA3IWKigoNGjToG2cIoW/Qr18/SX/9Rtrt9gBvAwAA2sPn8yk2Ntb6e/ybEELfoPnXYXa7nRACAKCLac/LWnixNAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAY/UI9AKQEpb8LtArAA+k0nWzAr0CgG6OZ4QAAICxCCEAAGCsDofQ//7v/+of//Ef1b9/f/Xq1UujR4/WqVOnrPNNTU3KycnRww8/rF69eik5OVmff/653zWuXLmitLQ02e12hYeHKyMjQ9evX/eb+cMf/qAnnnhCYWFhio2N1dq1a+/YZdeuXRoxYoTCwsI0evRo/f73v/c7355dAACAuToUQlevXtWkSZPUs2dP7du3T3/84x+1fv16PfTQQ9bM2rVrtXnzZuXl5en48ePq06ePXC6Xbty4Yc2kpaXp3LlzOnDggPbu3asjR45o3rx51nmfz6cpU6ZoyJAhKi0t1bp167Rq1Sq988471syxY8f04osvKiMjQ59++qmmTp2qqVOn6uzZsx3aBQAAmCuoqampqb3Dy5cv19GjR/Xxxx+3er6pqUkxMTF67bXX9Prrr0uSvF6voqKilJ+fr5kzZ+qzzz5TXFycTp48qcTERElSUVGRnn32WX355ZeKiYnR1q1b9dOf/lQej0ehoaHWY+/Zs0fnz5+XJM2YMUO1tbXau3ev9fgTJ05UfHy88vLy2rVLW3w+nxwOh7xer+x2e3u/TR3Gi6WB1vFiaQB3oyN/f3foGaEPPvhAiYmJ+od/+AdFRkbqu9/9rn7zm99Y5y9evCiPx6Pk5GTrmMPhUFJSkkpKSiRJJSUlCg8PtyJIkpKTkxUcHKzjx49bM08++aQVQZLkcrl04cIFXb161Zpp+TjNM82P055dbldXVyefz+d3AwAA3VeHQui///u/tXXrVj3yyCPav3+/FixYoB//+Mfavn27JMnj8UiSoqKi/O4XFRVlnfN4PIqMjPQ736NHD0VERPjNtHaNlo/xdTMtz7e1y+3WrFkjh8Nh3WJjY9v6lgAAgC6sQyHU2NiocePG6Y033tB3v/tdzZs3T3PnzlVeXl5n7XdfZWdny+v1WreKiopArwQAADpRh0Lo4YcfVlxcnN+xkSNHqry8XJIUHR0tSaqqqvKbqaqqss5FR0erurra7/ytW7d05coVv5nWrtHyMb5upuX5tna5nc1mk91u97sBAIDuq0MhNGnSJF24cMHv2J/+9CcNGTJEkjRs2DBFR0eruLjYOu/z+XT8+HE5nU5JktPpVE1NjUpLS62ZgwcPqrGxUUlJSdbMkSNHdPPmTWvmwIEDeuyxx6x3qDmdTr/HaZ5pfpz27AIAAMzWoRBavHixPvnkE73xxhv64osvtGPHDr3zzjvKzMyUJAUFBWnRokX65S9/qQ8++EBnzpzRrFmzFBMTo6lTp0r66zNITz/9tObOnasTJ07o6NGjysrK0syZMxUTEyNJ+uEPf6jQ0FBlZGTo3LlzKigo0KZNm+R2u61dXn31VRUVFWn9+vU6f/68Vq1apVOnTikrK6vduwAAALN16N8aGz9+vHbv3q3s7GytXr1aw4YN08aNG5WWlmbNLF26VLW1tZo3b55qamr0+OOPq6ioSGFhYdbMe++9p6ysLD311FMKDg5WamqqNm/ebJ13OBz66KOPlJmZqYSEBA0YMEA5OTl+nzX0ve99Tzt27NCKFSv0k5/8RI888oj27NmjUaNGdWgXAABgrg59jpBp+BwhILD4HCEAd6PTPkcIAACgOyGEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYKwOhdCqVasUFBTkdxsxYoR1/saNG8rMzFT//v3Vt29fpaamqqqqyu8a5eXlSklJUe/evRUZGaklS5bo1q1bfjOHDh3SuHHjZLPZNHz4cOXn59+xy5YtWzR06FCFhYUpKSlJJ06c8Dvfnl0AAIDZOvyM0He+8x395S9/sW7/+Z//aZ1bvHixPvzwQ+3atUuHDx9WZWWlpk2bZp1vaGhQSkqK6uvrdezYMW3fvl35+fnKycmxZi5evKiUlBRNnjxZZWVlWrRokV555RXt37/fmikoKJDb7dbKlSt1+vRpjR07Vi6XS9XV1e3eBQAAIKipqampvcOrVq3Snj17VFZWdsc5r9ergQMHaseOHZo+fbok6fz58xo5cqRKSko0ceJE7du3T88995wqKysVFRUlScrLy9OyZct06dIlhYaGatmyZSosLNTZs2eta8+cOVM1NTUqKiqSJCUlJWn8+PHKzc2VJDU2Nio2NlYLFy7U8uXL27VLa+rq6lRXV2d97fP5FBsbK6/XK7vd3t5vU4clLPldp10b6MpK180K9AoAuiCfzyeHw9Guv787/IzQ559/rpiYGP3N3/yN0tLSVF5eLkkqLS3VzZs3lZycbM2OGDFCgwcPVklJiSSppKREo0ePtiJIklwul3w+n86dO2fNtLxG80zzNerr61VaWuo3ExwcrOTkZGumPbu0Zs2aNXI4HNYtNja2o98eAADQhXQohJKSkpSfn6+ioiJt3bpVFy9e1BNPPKFr167J4/EoNDRU4eHhfveJioqSx+ORJHk8Hr8Iaj7ffO6bZnw+n7766itdvnxZDQ0Nrc60vEZbu7QmOztbXq/XulVUVLTvGwMAALqkHh0ZfuaZZ6z/HjNmjJKSkjRkyBDt3LlTvXr1+taXu99sNptsNlug1wAAAPfJPb19Pjw8XI8++qi++OILRUdHq76+XjU1NX4zVVVVio6OliRFR0ff8c6t5q/bmrHb7erVq5cGDBigkJCQVmdaXqOtXQAAAO4phK5fv64///nPevjhh5WQkKCePXuquLjYOn/hwgWVl5fL6XRKkpxOp86cOeP37q4DBw7IbrcrLi7Omml5jeaZ5muEhoYqISHBb6axsVHFxcXWTHt2AQAA6NCvxl5//XU9//zzGjJkiCorK7Vy5UqFhIToxRdflMPhUEZGhtxutyIiImS327Vw4UI5nU7rXVpTpkxRXFycXnrpJa1du1Yej0crVqxQZmam9Sup+fPnKzc3V0uXLtWcOXN08OBB7dy5U4WFhdYebrdb6enpSkxM1IQJE7Rx40bV1tZq9uzZktSuXQAAADoUQl9++aVefPFF/d///Z8GDhyoxx9/XJ988okGDhwoSdqwYYOCg4OVmpqquro6uVwuvf3229b9Q0JCtHfvXi1YsEBOp1N9+vRRenq6Vq9ebc0MGzZMhYWFWrx4sTZt2qRBgwZp27Ztcrlc1syMGTN06dIl5eTkyOPxKD4+XkVFRX4voG5rFwAAgA59jpBpOvI5BPeCzxECWsfnCAG4G536OUIAAADdBSEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGP1CPQCANCdJSz5XaBXAB5IpetmBXoFSTwjBAAADEYIAQAAY91TCL355psKCgrSokWLrGM3btxQZmam+vfvr759+yo1NVVVVVV+9ysvL1dKSop69+6tyMhILVmyRLdu3fKbOXTokMaNGyebzabhw4crPz//jsffsmWLhg4dqrCwMCUlJenEiRN+59uzCwAAMNddh9DJkyf1T//0TxozZozf8cWLF+vDDz/Url27dPjwYVVWVmratGnW+YaGBqWkpKi+vl7Hjh3T9u3blZ+fr5ycHGvm4sWLSklJ0eTJk1VWVqZFixbplVde0f79+62ZgoICud1urVy5UqdPn9bYsWPlcrlUXV3d7l0AAIDZ7iqErl+/rrS0NP3mN7/RQw89ZB33er3653/+Z7311lv6/ve/r4SEBL377rs6duyYPvnkE0nSRx99pD/+8Y/6l3/5F8XHx+uZZ57RL37xC23ZskX19fWSpLy8PA0bNkzr16/XyJEjlZWVpenTp2vDhg3WY7311luaO3euZs+erbi4OOXl5al379767W9/2+5dAACA2e4qhDIzM5WSkqLk5GS/46Wlpbp586bf8REjRmjw4MEqKSmRJJWUlGj06NGKioqyZlwul3w+n86dO2fN3H5tl8tlXaO+vl6lpaV+M8HBwUpOTrZm2rPL7erq6uTz+fxuAACg++rw2+fff/99nT59WidPnrzjnMfjUWhoqMLDw/2OR0VFyePxWDMtI6j5fPO5b5rx+Xz66quvdPXqVTU0NLQ6c/78+Xbvcrs1a9bo5z//+Tf86QEAQHfSoWeEKioq9Oqrr+q9995TWFhYZ+0UMNnZ2fJ6vdatoqIi0CsBAIBO1KEQKi0tVXV1tcaNG6cePXqoR48eOnz4sDZv3qwePXooKipK9fX1qqmp8btfVVWVoqOjJUnR0dF3vHOr+eu2Zux2u3r16qUBAwYoJCSk1ZmW12hrl9vZbDbZ7Xa/GwAA6L46FEJPPfWUzpw5o7KyMuuWmJiotLQ067979uyp4uJi6z4XLlxQeXm5nE6nJMnpdOrMmTN+7+46cOCA7Ha74uLirJmW12ieab5GaGioEhIS/GYaGxtVXFxszSQkJLS5CwAAMFuHXiPUr18/jRo1yu9Ynz591L9/f+t4RkaG3G63IiIiZLfbtXDhQjmdTk2cOFGSNGXKFMXFxemll17S2rVr5fF4tGLFCmVmZspms0mS5s+fr9zcXC1dulRz5szRwYMHtXPnThUWFlqP63a7lZ6ersTERE2YMEEbN25UbW2tZs+eLUlyOBxt7gIAAMz2rf9bYxs2bFBwcLBSU1NVV1cnl8ult99+2zofEhKivXv3asGCBXI6nerTp4/S09O1evVqa2bYsGEqLCzU4sWLtWnTJg0aNEjbtm2Ty+WyZmbMmKFLly4pJydHHo9H8fHxKioq8nsBdVu7AAAAswU1NTU1BXqJB5XP55PD4ZDX6+3U1wvxjzICrXtQ/lHGe8HPN9C6zvz57sjf3/xbYwAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIzVoRDaunWrxowZI7vdLrvdLqfTqX379lnnb9y4oczMTPXv3199+/ZVamqqqqqq/K5RXl6ulJQU9e7dW5GRkVqyZIlu3brlN3Po0CGNGzdONptNw4cPV35+/h27bNmyRUOHDlVYWJiSkpJ04sQJv/Pt2QUAAJitQyE0aNAgvfnmmyotLdWpU6f0/e9/Xy+88ILOnTsnSVq8eLE+/PBD7dq1S4cPH1ZlZaWmTZtm3b+hoUEpKSmqr6/XsWPHtH37duXn5ysnJ8eauXjxolJSUjR58mSVlZVp0aJFeuWVV7R//35rpqCgQG63WytXrtTp06c1duxYuVwuVVdXWzNt7QIAABDU1NTUdC8XiIiI0Lp16zR9+nQNHDhQO3bs0PTp0yVJ58+f18iRI1VSUqKJEydq3759eu6551RZWamoqChJUl5enpYtW6ZLly4pNDRUy5YtU2Fhoc6ePWs9xsyZM1VTU6OioiJJUlJSksaPH6/c3FxJUmNjo2JjY7Vw4UItX75cXq+3zV1aU1dXp7q6Outrn8+n2NhYeb1e2e32e/k2faOEJb/rtGsDXVnpulmBXuGe8fMNtK4zf759Pp8cDke7/v6+69cINTQ06P3331dtba2cTqdKS0t18+ZNJScnWzMjRozQ4MGDVVJSIkkqKSnR6NGjrQiSJJfLJZ/PZz2rVFJS4neN5pnma9TX16u0tNRvJjg4WMnJydZMe3ZpzZo1a+RwOKxbbGzs3X57AABAF9DhEDpz5oz69u0rm82m+fPna/fu3YqLi5PH41FoaKjCw8P95qOiouTxeCRJHo/HL4Kazzef+6YZn8+nr776SpcvX1ZDQ0OrMy2v0dYurcnOzpbX67VuFRUV7fumAACALqlHR+/w2GOPqaysTF6vV//6r/+q9PR0HT58uDN2u+9sNptsNlug1wAAAPdJh0MoNDRUw4cPlyQlJCTo5MmT2rRpk2bMmKH6+nrV1NT4PRNTVVWl6OhoSVJ0dPQd7+5qfidXy5nb391VVVUlu92uXr16KSQkRCEhIa3OtLxGW7sAAADc8+cINTY2qq6uTgkJCerZs6eKi4utcxcuXFB5ebmcTqckyel06syZM37v7jpw4IDsdrvi4uKsmZbXaJ5pvkZoaKgSEhL8ZhobG1VcXGzNtGcXAACADj0jlJ2drWeeeUaDBw/WtWvXtGPHDh06dEj79++Xw+FQRkaG3G63IiIiZLfbtXDhQjmdTutdWlOmTFFcXJxeeuklrV27Vh6PRytWrFBmZqb1K6n58+crNzdXS5cu1Zw5c3Tw4EHt3LlThYWF1h5ut1vp6elKTEzUhAkTtHHjRtXW1mr27NmS1K5dAAAAOhRC1dXVmjVrlv7yl7/I4XBozJgx2r9/v/7u7/5OkrRhwwYFBwcrNTVVdXV1crlcevvtt637h4SEaO/evVqwYIGcTqf69Omj9PR0rV692poZNmyYCgsLtXjxYm3atEmDBg3Stm3b5HK5rJkZM2bo0qVLysnJkcfjUXx8vIqKivxeQN3WLgAAAPf8OULdWUc+h+Be8DkjQOv4HCGg++rynyMEAADQ1RFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFgdCqE1a9Zo/Pjx6tevnyIjIzV16lRduHDBb+bGjRvKzMxU//791bdvX6Wmpqqqqspvpry8XCkpKerdu7ciIyO1ZMkS3bp1y2/m0KFDGjdunGw2m4YPH678/Pw79tmyZYuGDh2qsLAwJSUl6cSJEx3eBQAAmKtDIXT48GFlZmbqk08+0YEDB3Tz5k1NmTJFtbW11szixYv14YcfateuXTp8+LAqKys1bdo063xDQ4NSUlJUX1+vY8eOafv27crPz1dOTo41c/HiRaWkpGjy5MkqKyvTokWL9Morr2j//v3WTEFBgdxut1auXKnTp09r7Nixcrlcqq6ubvcuAADAbEFNTU1Nd3vnS5cuKTIyUocPH9aTTz4pr9ergQMHaseOHZo+fbok6fz58xo5cqRKSko0ceJE7du3T88995wqKysVFRUlScrLy9OyZct06dIlhYaGatmyZSosLNTZs2etx5o5c6ZqampUVFQkSUpKStL48eOVm5srSWpsbFRsbKwWLlyo5cuXt2uXtvh8PjkcDnm9Xtnt9rv9NrUpYcnvOu3aQFdWum5WoFe4Z/x8A63rzJ/vjvz9fU+vEfJ6vZKkiIgISVJpaalu3ryp5ORka2bEiBEaPHiwSkpKJEklJSUaPXq0FUGS5HK55PP5dO7cOWum5TWaZ5qvUV9fr9LSUr+Z4OBgJScnWzPt2eV2dXV18vl8fjcAANB93XUINTY2atGiRZo0aZJGjRolSfJ4PAoNDVV4eLjfbFRUlDwejzXTMoKazzef+6YZn8+nr776SpcvX1ZDQ0OrMy2v0dYut1uzZo0cDod1i42Nbed3AwAAdEV3HUKZmZk6e/as3n///W9zn4DKzs6W1+u1bhUVFYFeCQAAdKIed3OnrKws7d27V0eOHNGgQYOs49HR0aqvr1dNTY3fMzFVVVWKjo62Zm5/d1fzO7laztz+7q6qqirZ7Xb16tVLISEhCgkJaXWm5TXa2uV2NptNNputA98JAADQlXXoGaGmpiZlZWVp9+7dOnjwoIYNG+Z3PiEhQT179lRxcbF17MKFCyovL5fT6ZQkOZ1OnTlzxu/dXQcOHJDdbldcXJw10/IazTPN1wgNDVVCQoLfTGNjo4qLi62Z9uwCAADM1qFnhDIzM7Vjxw79+7//u/r162e91sbhcKhXr15yOBzKyMiQ2+1WRESE7Ha7Fi5cKKfTab1La8qUKYqLi9NLL72ktWvXyuPxaMWKFcrMzLSejZk/f75yc3O1dOlSzZkzRwcPHtTOnTtVWFho7eJ2u5Wenq7ExERNmDBBGzduVG1trWbPnm3t1NYuAADAbB0Koa1bt0qS/vZv/9bv+LvvvquXX35ZkrRhwwYFBwcrNTVVdXV1crlcevvtt63ZkJAQ7d27VwsWLJDT6VSfPn2Unp6u1atXWzPDhg1TYWGhFi9erE2bNmnQoEHatm2bXC6XNTNjxgxdunRJOTk58ng8io+PV1FRkd8LqNvaBQAAmO2ePkeou+NzhIDA4nOEgO6rW3yOEAAAQFdGCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYHQ6hI0eO6Pnnn1dMTIyCgoK0Z88ev/NNTU3KycnRww8/rF69eik5OVmff/6538yVK1eUlpYmu92u8PBwZWRk6Pr1634zf/jDH/TEE08oLCxMsbGxWrt27R277Nq1SyNGjFBYWJhGjx6t3//+9x3eBQAAmKvDIVRbW6uxY8dqy5YtrZ5fu3atNm/erLy8PB0/flx9+vSRy+XSjRs3rJm0tDSdO3dOBw4c0N69e3XkyBHNmzfPOu/z+TRlyhQNGTJEpaWlWrdunVatWqV33nnHmjl27JhefPFFZWRk6NNPP9XUqVM1depUnT17tkO7AAAAcwU1NTU13fWdg4K0e/duTZ06VdJfn4GJiYnRa6+9ptdff12S5PV6FRUVpfz8fM2cOVOfffaZ4uLidPLkSSUmJkqSioqK9Oyzz+rLL79UTEyMtm7dqp/+9KfyeDwKDQ2VJC1fvlx79uzR+fPnJUkzZsxQbW2t9u7da+0zceJExcfHKy8vr1273K6urk51dXXW1z6fT7GxsfJ6vbLb7Xf7bWpTwpLfddq1ga6sdN2sQK9wz/j5BlrXmT/fPp9PDoejXX9/f6uvEbp48aI8Ho+Sk5OtYw6HQ0lJSSopKZEklZSUKDw83IogSUpOTlZwcLCOHz9uzTz55JNWBEmSy+XShQsXdPXqVWum5eM0zzQ/Tnt2ud2aNWvkcDisW2xs7L18OwAAwAPuWw0hj8cjSYqKivI7HhUVZZ3zeDyKjIz0O9+jRw9FRET4zbR2jZaP8XUzLc+3tcvtsrOz5fV6rVtFRUU7/tQAAKCr6hHoBR4kNptNNpst0GsAAID75Ft9Rig6OlqSVFVV5Xe8qqrKOhcdHa3q6mq/87du3dKVK1f8Zlq7RsvH+LqZlufb2gUAAJjtWw2hYcOGKTo6WsXFxdYxn8+n48ePy+l0SpKcTqdqampUWlpqzRw8eFCNjY1KSkqyZo4cOaKbN29aMwcOHNBjjz2mhx56yJpp+TjNM82P055dAACA2TocQtevX1dZWZnKysok/fVFyWVlZSovL1dQUJAWLVqkX/7yl/rggw905swZzZo1SzExMdY7y0aOHKmnn35ac+fO1YkTJ3T06FFlZWVp5syZiomJkST98Ic/VGhoqDIyMnTu3DkVFBRo06ZNcrvd1h6vvvqqioqKtH79ep0/f16rVq3SqVOnlJWVJUnt2gUAAJitw68ROnXqlCZPnmx93Rwn6enpys/P19KlS1VbW6t58+appqZGjz/+uIqKihQWFmbd57333lNWVpaeeuopBQcHKzU1VZs3b7bOOxwOffTRR8rMzFRCQoIGDBignJwcv88a+t73vqcdO3ZoxYoV+slPfqJHHnlEe/bs0ahRo6yZ9uwCAADMdU+fI9TddeRzCO4FnzMCtI7PEQK6r275OUIAAABdCSEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjGVECG3ZskVDhw5VWFiYkpKSdOLEiUCvBAAAHgDdPoQKCgrkdru1cuVKnT59WmPHjpXL5VJ1dXWgVwMAAAHW7UPorbfe0ty5czV79mzFxcUpLy9PvXv31m9/+9tArwYAAAKsR6AX6Ez19fUqLS1Vdna2dSw4OFjJyckqKSm5Y76urk51dXXW116vV5Lk8/k6dc+Guq869fpAV9XZP3v3Az/fQOs68+e7+dpNTU1tznbrELp8+bIaGhoUFRXldzwqKkrnz5+/Y37NmjX6+c9/fsfx2NjYTtsRwNdz/Hp+oFcA0Enux8/3tWvX5HA4vnGmW4dQR2VnZ8vtdltfNzY26sqVK+rfv7+CgoICuBnuB5/Pp9jYWFVUVMhutwd6HQDfIn6+zdLU1KRr164pJiamzdluHUIDBgxQSEiIqqqq/I5XVVUpOjr6jnmbzSabzeZ3LDw8vDNXxAPIbrfzP0qgm+Ln2xxtPRPUrFu/WDo0NFQJCQkqLi62jjU2Nqq4uFhOpzOAmwEAgAdBt35GSJLcbrfS09OVmJioCRMmaOPGjaqtrdXs2bMDvRoAAAiwbh9CM2bM0KVLl5STkyOPx6P4+HgVFRXd8QJqwGazaeXKlXf8ehRA18fPN75OUFN73lsGAADQDXXr1wgBAAB8E0IIAAAYixACAADGIoQAAICxCCHg/9uyZYuGDh2qsLAwJSUl6cSJE4FeCcA9OnLkiJ5//nnFxMQoKChIe/bsCfRKeMAQQoCkgoICud1urVy5UqdPn9bYsWPlcrlUXV0d6NUA3IPa2lqNHTtWW7ZsCfQqeEDx9nlAUlJSksaPH6/c3FxJf/0E8tjYWC1cuFDLly8P8HYAvg1BQUHavXu3pk6dGuhV8ADhGSEYr76+XqWlpUpOTraOBQcHKzk5WSUlJQHcDADQ2QghGO/y5ctqaGi449PGo6Ki5PF4ArQVAOB+IIQAAICxCCEYb8CAAQoJCVFVVZXf8aqqKkVHRwdoKwDA/UAIwXihoaFKSEhQcXGxdayxsVHFxcVyOp0B3AwA0Nm6/b8+D7SH2+1Wenq6EhMTNWHCBG3cuFG1tbWaPXt2oFcDcA+uX7+uL774wvr64sWLKisrU0REhAYPHhzAzfCg4O3zwP+Xm5urdevWyePxKD4+Xps3b1ZSUlKg1wJwDw4dOqTJkyffcTw9PV35+fn3fyE8cAghAABgLF4jBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAHosl5++WUFBQXpzTff9Du+Z88eBQUFBWgrAF0JIQSgSwsLC9OvfvUrXb16NdCrAOiCCCEAXVpycrKio6O1Zs2ar535t3/7N33nO9+RzWbT0KFDtX79er/zQ4cO1RtvvKE5c+aoX79+Gjx4sN555x2/mYqKCv3gBz9QeHi4IiIi9MILL+h//ud/OuOPBOA+IoQAdGkhISF644039Otf/1pffvnlHedLS0v1gx/8QDNnztSZM2e0atUq/exnP7vjXx5fv369EhMT9emnn+pHP/qRFixYoAsXLkiSbt68KZfLpX79+unjjz/W0aNH1bdvXz399NOqr6+/H39MAJ2EEALQ5f393/+94uPjtXLlyjvOvfXWW3rqqaf0s5/9TI8++qhefvllZWVlad26dX5zzz77rH70ox9p+PDhWrZsmQYMGKD/+I//kCQVFBSosbFR27Zt0+jRozVy5Ei9++67Ki8v16FDh+7HHxFAJyGEAHQLv/rVr7R9+3Z99tlnfsc/++wzTZo0ye/YpEmT9Pnnn6uhocE6NmbMGOu/g4KCFB0drerqaknSf/3Xf+mLL75Qv3791LdvX/Xt21cRERG6ceOG/vznP3finwpAZ+sR6AUA4Nvw5JNPyuVyKTs7Wy+//HKH79+zZ0+/r4OCgtTY2ChJun79uhISEvTee+/dcb+BAwfe1b4AHgyEEIBu480331R8fLwee+wx69jIkSN19OhRv7mjR4/q0UcfVUhISLuuO27cOBUUFCgyMlJ2u/1b3RlAYPGrMQDdxujRo5WWlqbNmzdbx1577TUVFxfrF7/4hf70pz9p+/btys3N1euvv97u66alpWnAgAF64YUX9PHHH+vixYs6dOiQfvzjH7f6Am0AXQchBKBbWb16tfUrLemvz+bs3LlT77//vkaNGqWcnBytXr26Q78+6927t44cOaLBgwdr2rRpGjlypDIyMnTjxg2eIQK6uKCmpqamQC8BAAAQCDwjBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFj/DzRjRFlSLBegAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns.histplot([page.is_local_temp for page in pages], discrete=True)\n",
    "# sns.histplot([page.mode for page in pages], discrete=True)\n",
    "\n",
    "# sns.histplot([page.relam for page in pages], discrete=True)\n",
    "data = pd.Series([page.relminmxid for page in pages])\n",
    "value_counts = data.value_counts()\n",
    "sns.barplot(x=value_counts.index, y=value_counts.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimal_res(pages, buffer, current_index):\n",
    "    res = [0] * (len(buffer))\n",
    "    if (current_index >= len(pages)):\n",
    "        print(f\"ERROR: current_index=={current_index} pages.size() == {len(pages)}\")\n",
    "\n",
    "    already_in_buf = any(buf.buffer == pages[current_index].buffer for buf in buffer)\n",
    "    if already_in_buf:\n",
    "        return res, -1\n",
    "    \n",
    "    empty_pages = [index for index, value in enumerate(buffer) if value.buffer == -1]\n",
    "    if len(empty_pages) > 0:\n",
    "        for i in empty_pages:\n",
    "            res[i] = 1 / len(empty_pages)\n",
    "        \n",
    "        return res, empty_pages[0]\n",
    "    \n",
    "    # victims_rates = optimal_results[current_index]\n",
    "    # victim_rates_sum = sum([victim[1] for victim in victims_rates])\n",
    "    # for victim, rate in victims_rates:\n",
    "    #     res[victim] = rate / victim_rates_sum\n",
    "    victims_rates = optimal_results[current_index]\n",
    "    res[victims_rates[0][0]] = 1\n",
    "\n",
    "    return res, victims_rates[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(pages, buffer, batch_start, batch_end):\n",
    "    pages_acc = torch.Tensor([list(asdict(page).values()) for page in pages[batch_start:batch_end]])\n",
    "\n",
    "    buffers = []\n",
    "    optimal_predictions = []\n",
    "    hit_fail_mask = []\n",
    "\n",
    "    for i in range(batch_start, batch_end):\n",
    "        buffers.append([value for obj in buffer for value in asdict(obj).values()])\n",
    "\n",
    "        res, victim = get_model_optimal_res(pages, buffer, i)\n",
    "        optimal_predictions.append(res)\n",
    "\n",
    "        if victim >= 0:\n",
    "            buffer[victim] = pages[i]\n",
    "            buffer[victim].hit = 1\n",
    "            pages_acc[i - batch_start][-1] = 0\n",
    "            hit_fail_mask.append(1)\n",
    "        else:\n",
    "            pages_acc[i - batch_start][-1] = 1\n",
    "            hit_fail_mask.append(0)\n",
    "    \n",
    "    return pages_acc, torch.Tensor(buffers), torch.Tensor(optimal_predictions), buffer, torch.tensor(hit_fail_mask, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PageAccModel(len(fields(Page)), 128, 512, BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [36:00<00:00,  2.16s/it, loss=3.485332713842392]\n",
      "  9%|▊         | 87/1001 [04:17<45:00,  2.95s/it, loss=3.2721244083053764]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_end \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m TRAIN_SIZE:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m pages_acc, buffers, optimal_predictions, buffer, hit_fail_mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimal_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(optimal_predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36mget_train_data\u001b[0;34m(pages, buffer, batch_start, batch_end)\u001b[0m\n\u001b[1;32m      6\u001b[0m hit_fail_mask \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_start, batch_end):\n\u001b[0;32m----> 9\u001b[0m     buffers\u001b[38;5;241m.\u001b[39mappend(\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     11\u001b[0m     res, victim \u001b[38;5;241m=\u001b[39m get_model_optimal_res(pages, buffer, i)\n\u001b[1;32m     12\u001b[0m     optimal_predictions\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m hit_fail_mask \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_start, batch_end):\n\u001b[0;32m----> 9\u001b[0m     buffers\u001b[38;5;241m.\u001b[39mappend([value \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m buffer \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m \u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[1;32m     11\u001b[0m     res, victim \u001b[38;5;241m=\u001b[39m get_model_optimal_res(pages, buffer, i)\n\u001b[1;32m     12\u001b[0m     optimal_predictions\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1275\u001b[0m, in \u001b[0;36masdict\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_dataclass_instance(obj):\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masdict() should be called on dataclass instances\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_asdict_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1282\u001b[0m, in \u001b[0;36m_asdict_inner\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1280\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields(obj):\n\u001b[0;32m-> 1282\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43m_asdict_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend((f\u001b[38;5;241m.\u001b[39mname, value))\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dict_factory(result)\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1279\u001b[0m, in \u001b[0;36m_asdict_inner\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_asdict_inner\u001b[39m(obj, dict_factory):\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_dataclass_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1280\u001b[0m         result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1281\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields(obj):\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1242\u001b[0m, in \u001b[0;36m_is_dataclass_instance\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;66;03m# Exclude pseudo-fields.  Note that fields is sorted by insertion\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;66;03m# order, so the order of the tuple is as the fields were defined.\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_field_type \u001b[38;5;129;01mis\u001b[39;00m _FIELD)\n\u001b[0;32m-> 1242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_is_dataclass_instance\u001b[39m(obj):\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if obj is an instance of a dataclass.\"\"\"\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(obj), _FIELDS)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(reduction='none') # Установим 'none' для получения потерь по каждому элементу\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "f = open(\"train.txt\", \"w\")\n",
    "\n",
    "h, c = None, None\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    buffer = [Page(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0)] * BUFFER_SIZE\n",
    "\n",
    "    loss_sum = 0\n",
    "    pbar = tqdm(range(0, TRAIN_SIZE, BATCH_SIZE))\n",
    "    for i in pbar:\n",
    "        batch_start = i\n",
    "        # batch_end = i + BATCH_SIZE if i + BATCH_SIZE < TRAIN_SIZE else TRAIN_SIZE\n",
    "        batch_end = i + BATCH_SIZE\n",
    "        if batch_end >= TRAIN_SIZE:\n",
    "            continue\n",
    "        pages_acc, buffers, optimal_predictions, buffer, hit_fail_mask = get_train_data(pages, buffer, batch_start, batch_end)\n",
    "        optimal_predictions = torch.argmax(optimal_predictions, dim=1)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        out, h, c = model.forward(pages_acc, buffers, h, c)\n",
    "\n",
    "        losses = loss(out, optimal_predictions)\n",
    "        masked_losses = losses[hit_fail_mask]\n",
    "        loss_value = masked_losses.mean()\n",
    "    \n",
    "        loss_value.backward()\n",
    "\n",
    "        f.write(\"=========================\\n\")\n",
    "        for i in range(len(hit_fail_mask)):\n",
    "            if hit_fail_mask[i]:\n",
    "                f.write(f\"{i}. ////\\n\")\n",
    "                f.write(f\"{out[i]}\\n\")\n",
    "                f.write(f\"{optimal_predictions[i]}\\n\")\n",
    "\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        f.write(f\"{name}: {param.grad.abs().mean()}\\n\")\n",
    "\n",
    "                f.write(\"////\\n\")\n",
    "        f.write(\"=========================\\n\")\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "        h = h.detach()\n",
    "        c = c.detach()\n",
    "\n",
    "        loss_sum += loss_value.item()\n",
    "        loss_avg = loss_sum / (batch_end // BATCH_SIZE)\n",
    "\n",
    "        pbar.set_postfix_str(f\"loss={loss_avg}\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PageAccModel(\n",
       "  (_page_acc_enc): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (_lstm): LSTM(64, 64, batch_first=True)\n",
       "  (_buf_page_enc): Sequential(\n",
       "    (0): Linear(in_features=352, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (_page_evict): Sequential(\n",
       "    (0): Linear(in_features=2112, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:00<00:00, 217.58it/s, loss=2.2899919626664142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
      "         0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
      "         0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
      "         0.0312, 0.0312, 0.0312, 0.0312, 0.0312]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
      "         0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
      "         0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
      "         0.0323, 0.0323, 0.0323, 0.0323, 0.0323]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333,\n",
      "         0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333,\n",
      "         0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333, 0.0333,\n",
      "         0.0333, 0.0333, 0.0333, 0.0333, 0.0333]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345,\n",
      "         0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345,\n",
      "         0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345,\n",
      "         0.0345, 0.0345, 0.0345, 0.0345, 0.0345]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357,\n",
      "         0.0357, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357,\n",
      "         0.0357, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357, 0.0357,\n",
      "         0.0357, 0.0357, 0.0357, 0.0357, 0.0357]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0370, 0.0370, 0.0370, 0.0370,\n",
      "         0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
      "         0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
      "         0.0370, 0.0370, 0.0370, 0.0370, 0.0370]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0385, 0.0385, 0.0385,\n",
      "         0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "         0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "         0.0385, 0.0385, 0.0385, 0.0385, 0.0385]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400, 0.0400,\n",
      "         0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400,\n",
      "         0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400,\n",
      "         0.0400, 0.0400, 0.0400, 0.0400, 0.0400]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0417,\n",
      "         0.0417, 0.0417, 0.0417, 0.0417, 0.0417, 0.0417, 0.0417, 0.0417, 0.0417,\n",
      "         0.0417, 0.0417, 0.0417, 0.0417, 0.0417, 0.0417, 0.0417, 0.0417, 0.0417,\n",
      "         0.0417, 0.0417, 0.0417, 0.0417, 0.0417]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0435,\n",
      "         0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0435, 0.0435,\n",
      "         0.0435, 0.0435, 0.0435, 0.0435, 0.0435]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455,\n",
      "         0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455,\n",
      "         0.0455, 0.0455, 0.0455, 0.0455, 0.0455]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
      "         0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
      "         0.0476, 0.0476, 0.0476, 0.0476, 0.0476]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
      "         0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
      "         0.0500, 0.0500, 0.0500, 0.0500, 0.0500]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526,\n",
      "         0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526,\n",
      "         0.0526, 0.0526, 0.0526, 0.0526, 0.0526]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0556, 0.0556, 0.0556, 0.0556,\n",
      "         0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556,\n",
      "         0.0556, 0.0556, 0.0556, 0.0556, 0.0556]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0588, 0.0588, 0.0588,\n",
      "         0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588,\n",
      "         0.0588, 0.0588, 0.0588, 0.0588, 0.0588]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0625, 0.0625,\n",
      "         0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
      "         0.0625, 0.0625, 0.0625, 0.0625, 0.0625]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667,\n",
      "         0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "         0.0667, 0.0667, 0.0667, 0.0667, 0.0667]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
      "         0.0714, 0.0714, 0.0714, 0.0714, 0.0714]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "         0.0769, 0.0769, 0.0769, 0.0769, 0.0769]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "         0.0833, 0.0833, 0.0833, 0.0833, 0.0833]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,\n",
      "         0.0909, 0.0909, 0.0909, 0.0909, 0.0909]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.1111, 0.1111, 0.1111,\n",
      "         0.1111, 0.1111, 0.1111, 0.1111, 0.1111]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.1429,\n",
      "         0.1429, 0.1429, 0.1429, 0.1429, 0.1429]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667,\n",
      "         0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.3333, 0.3333, 0.3333]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.5000, 0.5000]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [00:00<00:00, 229.04it/s, loss=1.4253738274176915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 234.76it/s, loss=1.5090804815292358]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n",
      "==================\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "h, c = None, None\n",
    "buffer = [Page(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)] * BUFFER_SIZE\n",
    "\n",
    "loss_sum = 0\n",
    "step = 1\n",
    "pbar = tqdm(range(0, 100, step))\n",
    "for i in pbar:\n",
    "    batch_start = i\n",
    "    # batch_end = i + BATCH_SIZE if i + BATCH_SIZE < TRAIN_SIZE else TRAIN_SIZE\n",
    "    batch_end = i + step \n",
    "    if batch_end >= TRAIN_SIZE:\n",
    "        continue\n",
    "    pages_acc, buffers, optimal_predictions, buffer, _ = get_train_data(pages, buffer, batch_start, batch_end)\n",
    "\n",
    "    out, h, c = model.forward(pages_acc, buffers, h, c)\n",
    "    print(\"==================\")\n",
    "    print(out)\n",
    "    print(optimal_predictions)\n",
    "    print(\"==================\")\n",
    "\n",
    "    loss_value = loss(out, optimal_predictions)\n",
    "\n",
    "    loss_sum += loss_value.item()\n",
    "    loss_avg = loss_sum / (batch_end // step)\n",
    "\n",
    "    pbar.set_postfix_str(f\"loss={loss_avg}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
