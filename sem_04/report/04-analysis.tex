\chapter{Проектирование и исследование метода}

\textbf{Кодировщик запроса обращения к странице} отвечает за скрытое представление атрибутов страницы, к которой происходит очередное обращение.
Схема кодировщика запроса обращения к странице изображена на рисунке \ref{img:page_acc}.
\includeimage
{page_acc} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.6\textwidth} % Ширина рисунка
{Схема кодировщика запроса обращения к странице} % Подпись рисунка

На вход кодировщика поступают $n$ атрибутов страницы.
Каждый атрибут может иметь $m_i$ возможных значений, где $i$ -- индекс атрибута.
Каждый атрибут представляется в виде вектора $a^{(i)}$ размерности $m_i$.
Для категориальных данных используется техника однозначного кодирования, а для числовых -- применяется хэш функция и к полученному результату применяется техника однозначного кодирования.
$W_1^{(i)}$ -- матрица обучаемых весов для скрытого представления i-го атрибута.
Вектор $z$ -- выходной вектор из сети.
$W_1$ -- матрица обучаемых весов, при помощи который получается результирующий вектор из скрытых представлений атрибутов сети.
В качестве функции активации на последнем слое используется функция Relu.
$d_f$ и $d_z$ являются настраиваемыми параметрами, которые отвечают за число нейронов, отвечающий за скрытое представление каждого атрибута, и число нейронов на выходном слое соответственно.

Работу сети можно описать с помощью выражений
\ref{formula:page_enc_1} - \ref{formula:page_enc_3}:
\begin{equation}\label{formula:page_enc_1}
	f^{(i)} = a^{(i)}W_1^{(i)} i \in \{1;n\},
\end{equation}

\begin{equation}\label{formula:page_enc_2}
	f = [f^{(1)}, f^{(2)}, ..., f^{(n)}],
\end{equation}

\begin{equation}\label{formula:page_enc_3}
	z = ReLU(W_1f^T + l_1),
\end{equation}
где $f$ является конкатенацией векторов скрытых состояний атрибутов страницы, а $l_1$ -- обучаемым вектором.

\textbf{Кодировщик страниц в буфере} нужен для скрытого представления каждой страницы в буфере.
Схема кодировщика представлена на рисунке \ref{img:buf_page_enc}
\includeimage
{buf_page_enc} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Схема кодировщика страниц в буфере} % Подпись рисунка

На вход кодировщика поступают $M$ страниц из буфера.
Каждая страница представляется в виде $n$ атрибутов.
Процесс обработки атрибутов для каждой страницы такой же, как и в кодировщике запроса обращения к странице.
Для каждой i-ой страницы в буфере вычисляется вектор $b_i$.
$d_b$ является настраиваемым параметром, который отвечает за размерность векторов $b_i$.

Для получения скрытого представления каждого атрибута и для вычисления закодированного представления страницы используется одни и те же матрицы весов $W_2^{(i)}$ и $W_2$ для всех страниц в буфере.
За счет этого матрицы весов не привязаны к конкретной позиции страницы в буфере и истории страниц на этой позиции.
При обратном распространении ошибки влияние веса из матрицы $W_2$ будет учитываться для всех векторов $b_i$.

Обозначим результат работы сумматора нейрона на выходном слое как $s_{ij}$.
Индексация в матрице $s$ совпадает с матрицей $b$.
Тогда для вычисления ошибки по весу $w_{ij}$ из матрицы $W_2$ на ребре, которое соединяет j-ый нейрон из второго слоя и i-ый нейрон из выходного слоя, используется выражение \ref{formula:buf_page_enc_err}:
\begin{equation}\label{formula:buf_page_enc_err}
	\frac{\delta E}{\delta w_{ij}} = \sum\limits_{k=1}^{M}\frac{\delta E}{\delta b_{ki}}\frac{\delta b_{ki}}{\delta s_{ki}} \frac{\delta s_{ki}}{\delta w_ij},
\end{equation}
где $E$ -- функция ошибки, $\frac{\delta E}{\delta b_{ki}}$ -- ошибка полученная со следующего слоя.

Функционирование кодировщика определяется выражениями
\ref{formula:buf_page_enc_1} - \ref{formula:buf_page_enc_3}:
\begin{equation}\label{formula:buf_page_enc_1}
	f^{(j,i)} = a^{(j,i)}W_2^{(i)} j \in \{1;M\} i \in \{1;n\},
\end{equation}

\begin{equation}\label{formula:buf_page_enc_2}
	f^{(j)} = [f^{(j,1)}, f^{(j,2)}, ..., f^{(j,n)}],
\end{equation}

\begin{equation}\label{formula:buf_page_enc_3}
	b_j = ReLU(W_2f^{(j)T} + l_2),
\end{equation}
где $f^{(j)}$ -- конкатенация скрытых представлений атрибутов для j-ой страницы в буфере, $b_j$ -- скрытое представление этой страницы, $l_2$ -- вектор обучаемых весов.

\textbf{Кодировщик истории обращений.} Для обновления истории обращений используется сеть LSTM.
На вход сети поступают результат работы кодировщика обращения к странице, предыдущий результат кодировщика истории обращений и предыдущее состояние ячейки.

Функционирование кодировщика определяется выражениями
\ref{formula:lstm_enc_1} - \ref{formula:lstm_enc_6}:
\begin{equation}\label{formula:lstm_enc_1}
	f_t = \sigma(W_f[h_{t-1}, z_t] + b_f),
\end{equation}

\begin{equation}\label{formula:lstm_enc_2}
	i_t = \sigma(W_i[h_{t-1}, z_t] + b_i),
\end{equation}

\begin{equation}\label{formula:lstm_enc_3}
	\hat{C_t} = \tanh(W_C[h_{t-1}, z_t] + b_C),
\end{equation}

\begin{equation}\label{formula:lstm_enc_4}
	C_t = f_t * C_{t-1} + i_t*\hat{C_t},
\end{equation}

\begin{equation}\label{formula:lstm_enc_5}
	o_t = \sigma(W_o[h_{t-1}, z_t] + b_o),
\end{equation}

\begin{equation}\label{formula:lstm_enc_6}
	h_t = o_t * \tanh(C_t),
\end{equation}

где $[h_{t-1}, z_t]$ -- конкатенация результата работы предыдущего слоя кодировщика истории и скрытого состояния, полученного из кодировщика обращения к странице, $W_f$ и $b_f$ -- матрица и вектор обучаемых весов, $f_t$ -- результат работы фильтра забывания, $i_t$ определяет, какие значения будут сохранены в ячейке, $\hat{C_t}$ -- новые значения кандидатов на попадание в ячейку, $W_i$, $W_C$, $b_i$, $b_c$ -- матрицы и вектора обучаемых весов, $C_t$ -- новое состояние ячейки, $C_{t-1}$ -- состояние ячейки на прошлом шаге, $h_t$ -- результат работы текущего слоя, $C_t$ -- состояние ячейки, $W_o$ и $b_o$ -- матрица и вектор обучаемых весов.
Вектора $h_t$ и $C_t$ имеют размерность $d_h$, где $d_h$ -- настраиваемый параметр.

\textbf{Модуль выбора страниц для замещения.}
На вход модуля поступают результаты работы кодировщика страниц в буфере и кодировщика истории обращений.
Для выбора страницы, которая будет удалена из буфера используется указательная нейронная сеть с механизмом внимания \cite{vinyals2015pointer}.

Нейронные сети с механизмом внимания -- это архитектуры, которые позволяют моделям динамически фокусироваться на наиболее релевантных частях входных данных при обработке информации.
Этот подход нашел применения в областях обработки естественного языка, компьютерного зрения и других задач, где важно учитывать контекст и зависимости между элементами последовательности.
В модуле выбора страниц для замещения контекстом является результат работы кодировщика истории, а элементами последовательности -- результаты работы кодировщика страниц в буфере.

Указательные сети -- архитектура сетей с механизмом внимания, предназначенная для решения задач, где выходные элементы представляют собой позиции в входной последовательности.
В указательных сетях механизм внимания используется как указатель на один из элементов входной последовательности, а не для создания контекстного вектора, как в классических моделях с механизмом внимания.

Схема модуля выбора страниц для замещения представлена на рисунке \ref{img:decision_maker}
\includeimage
{decision_maker} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Схема модуля выбора страниц для замещения} % Подпись рисунка

$W_4$ -- матрица обучаемых весов, которая используется для преобразования вектора $h$, полученного из кодировщика истории в вектор контекста размерности $d_v$. $d_v$ является настраиваемым параметром модели.

$W_3$ -- матрица обучаемых весов, которая используется для преобразования закодированного состояния очередной страницы в буфере в вектор размерности $d_v$, который будет использован в функции внимания.

$v$ -- вектор обучаемых весов, который который используется при вычислении функции внимания.

Функционирование модуля выбора страниц для замещения определяется выражениями \ref{formula:evict_1} - \ref{formula:evict_3}:
\begin{equation}\label{formula:evict_1}
	u_i = v * \tanh(W_3 b_i^T + W_4 h^T), i \in \{1; M\},
\end{equation}

\begin{equation}\label{formula:evict_2}
	p_i = SoftMax(u_i),
\end{equation}

\begin{equation}\label{formula:evict_3}
	r = \arg\max_{i} p_i,
\end{equation}
где $M$ -- число страниц в буфере, $u_i$ -- результат функции внимания для i-ой страницы в буфере, $\arg\max_{i} p_i$ -- функция, которая возвращает индекс максимального элемента в последовательности, $r$ -- результат работы спроектированного метода замещения страниц.

\section{Обучение и тестирование модели}

Обучение модели проводилось на машине с процессором Intel Core i9-10900, 64 гигабайтами оперативной памяти и графической картой NVIDIA GeForce RTX 3080 с 16 гигабайтами памяти типа GDDR6.

В качестве оптимизатора функции потерь был выбран Adam, так как он автоматически адаптирует скорость обучения для каждого параметра в зависимости от его градиента, что позволяет более эффективно использовать скорость обучения и ускоряет сходимость.

Обучение модели проводилось на протяжении 100 эпох.
После прохождения каждой эпохи веса модели сохранялись в файл и вычислялась точность модели на тестовой выборке.
Была выбрана модель с наивысшей точностью на тестовой выборке.

Графики зависимостей точности модели на тестовой и обучающей выборках от номера эпохи обучения приведены на рисунке \ref{img:training_512}.

\includeimage
{training_512} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Точность при обучении модели на тренировочной и тестовой выборках} % Подпись рисунка

Наивысшая точность модели была получена на 97 эпохе -- 45.2 процента.
Точность на обучающей выборке составила 63 процента.

\section{Подбор параметров сети}
Для оценки разработанного метода вводятся следующие метрики качества:
\begin{itemize}
	\item коэффициент попадания -- отношение числа обращений к страницам, которые уже загружены в буфер, к общему числу обращений;
	\item коэффициент совпадения -- отношение количества совпавших с оптимальным алгоритмов кандидатов на замещение с общим числом запросов поиска страниц для вытеснения.
\end{itemize}

Размер скрытых слоев модели подбирался экспериментально.
Графики зависимости коэффициента совпадения в зависимости от эпохи обучения для различных размеров скрытых слоев на обучающей и тестовой выборках представлены на рисунках \ref{img:test_sizes_train} и \ref{img:test_sizes_test} соответственно.
\includeimage
{test_sizes_train} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Точность модели для различных размеров скрытых слое на тренировочной выборке} % Подпись рисунка

\includeimage
{test_sizes_test} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Точность модели для различных размеров скрытых слое на тестовой выборке} % Подпись рисунка

Исходя из полученных результатов, настраиваемые параметры модели: $d_z$, $d_b$, $d_h$ и $d_v$ были выбраны равными 448, а $d_f$ -- 32.

\section{Сравнение с аналогами}
Сравнение коэффициентов попадания для разработанного метода и существующих аналогов приведено на рисунке \ref{img:hits_comp}.

\includeimage
{hits_comp} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.8\textwidth} % Ширина рисунка
{Коэффициент попадания в зависимости от числа обращений для различных методов} % Подпись рисунка

Из графиков видно, что коэффициент попадания для разработанного метода в среднем на 0.03 выше чем для алгоритма clock, который в настоящее время используется в PostgreSQL.
Также коэффициент попадания для разработанного метода на 0.08 ниже, чем у оптимального алгоритма.
Таким образом, разработанный метод лучше существующий аналогов, но все еще имеет возможность для улучшения.

Коэффициент попадания для существующих аналогов ниже одного процента.