{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import seaborn as sns\n",
    "import threading\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "from dataclasses import dataclass, fields, asdict\n",
    "from abc import ABC, abstractmethod\n",
    "from model.model import PageAccModel\n",
    "from logfile_reader import read_pages, save_pages_accs, read_optimal_results, Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PART = 0.7\n",
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = read_pages(\"train_data/tpcc_logfile\")\n",
    "train_size = int(len(pages) * TRAIN_PART)\n",
    "train_pages = pages[:train_size]\n",
    "del pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_results = read_optimal_results(\"train_data/tpcc_logfile_train_victims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1547962\n"
     ]
    }
   ],
   "source": [
    "assert(len(optimal_results) == len(train_pages))\n",
    "print(len(train_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimal_res(pages, buffer, current_index):\n",
    "    res = [0] * (len(buffer))\n",
    "    if (current_index >= len(pages)):\n",
    "        print(f\"ERROR: current_index=={current_index} pages.size() == {len(pages)}\")\n",
    "\n",
    "    if len(optimal_results[current_index]) == 0:\n",
    "        page_in_buffer = next(filter(lambda el: el[1].get_page_id() == pages[current_index].get_page_id(), enumerate(buffer)), None)\n",
    "        return res, page_in_buffer[0]\n",
    "    \n",
    "    victims_rates = optimal_results[current_index]\n",
    "    res[victims_rates[0][0]] = 1\n",
    "\n",
    "    return res, victims_rates[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(pages, buffer, batch_start, batch_end):\n",
    "    pages_acc = torch.Tensor([list(asdict(page).values()) for page in pages[batch_start:batch_end]])\n",
    "\n",
    "    buffers = []\n",
    "    optimal_predictions = []\n",
    "    hit_fail_mask = []\n",
    "\n",
    "    for i in range(batch_start, batch_end):\n",
    "        buffers.append([value for obj in buffer for value in asdict(obj).values()])\n",
    "\n",
    "        res, victim = get_model_optimal_res(pages, buffer, i)\n",
    "        optimal_predictions.append(res)\n",
    "\n",
    "        if sum(res) > 0:\n",
    "            buffer[victim] = deepcopy(pages[i])\n",
    "            buffer[victim].hit = victim\n",
    "            pages_acc[i - batch_start][-1] = BUFFER_SIZE\n",
    "            hit_fail_mask.append(1)\n",
    "        else:\n",
    "            pages_acc[i - batch_start][-1] = victim\n",
    "            hit_fail_mask.append(0)\n",
    "    \n",
    "    return pages_acc, torch.Tensor(buffers), torch.Tensor(optimal_predictions), buffer, torch.tensor(hit_fail_mask, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PageAccModel(len(fields(Page)), 256, 512, BUFFER_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"drive/MyDrive/model1.pth\", map_location=device, weights_only=True))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(reduction='none') # Установим 'none' для получения потерь по каждому элементу\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(torch.load(\"drive/MyDrive/opt1.pth\", map_location=device, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 377/773981 [00:20<11:26:40, 18.78it/s, loss=0.7860872256977173]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_end \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_pages):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m pages_acc, buffers, optimal_predictions, buffer, hit_fail_mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimal_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(optimal_predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mget_train_data\u001b[0;34m(pages, buffer, batch_start, batch_end)\u001b[0m\n\u001b[1;32m      6\u001b[0m hit_fail_mask \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_start, batch_end):\n\u001b[0;32m----> 9\u001b[0m     buffers\u001b[38;5;241m.\u001b[39mappend(\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     11\u001b[0m     res, victim \u001b[38;5;241m=\u001b[39m get_model_optimal_res(pages, buffer, i)\n\u001b[1;32m     12\u001b[0m     optimal_predictions\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m hit_fail_mask \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_start, batch_end):\n\u001b[0;32m----> 9\u001b[0m     buffers\u001b[38;5;241m.\u001b[39mappend([value \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m buffer \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m \u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[1;32m     11\u001b[0m     res, victim \u001b[38;5;241m=\u001b[39m get_model_optimal_res(pages, buffer, i)\n\u001b[1;32m     12\u001b[0m     optimal_predictions\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1275\u001b[0m, in \u001b[0;36masdict\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_dataclass_instance(obj):\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masdict() should be called on dataclass instances\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_asdict_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1282\u001b[0m, in \u001b[0;36m_asdict_inner\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1280\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields(obj):\n\u001b[0;32m-> 1282\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43m_asdict_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend((f\u001b[38;5;241m.\u001b[39mname, value))\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dict_factory(result)\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1316\u001b[0m, in \u001b[0;36m_asdict_inner\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)((_asdict_inner(k, dict_factory),\n\u001b[1;32m   1313\u001b[0m                       _asdict_inner(v, dict_factory))\n\u001b[1;32m   1314\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/copy.py:128\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    124\u001b[0m     d[PyStringMap] \u001b[38;5;241m=\u001b[39m PyStringMap\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m d, t\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeepcopy\u001b[39m(x, memo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _nil\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deep copy operation on arbitrary Python objects.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    See the module's __doc__ string for more info.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# f = open(\"train.txt\", \"w\")\n",
    "\n",
    "h, c = None, None\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    buffer = [Page(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0)] * BUFFER_SIZE\n",
    "\n",
    "    loss_sum = 0\n",
    "    pbar = tqdm(range(0, len(train_pages), BATCH_SIZE))\n",
    "    for i in pbar:\n",
    "        batch_start = i\n",
    "        # batch_end = i + BATCH_SIZE if i + BATCH_SIZE < TRAIN_SIZE else TRAIN_SIZE\n",
    "        batch_end = i + BATCH_SIZE\n",
    "        if batch_end >= len(train_pages):\n",
    "            continue\n",
    "        pages_acc, buffers, optimal_predictions, buffer, hit_fail_mask = get_train_data(train_pages, buffer, batch_start, batch_end)\n",
    "        optimal_predictions = torch.argmax(optimal_predictions, dim=1)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out, h, c = model.forward(pages_acc.to(device), buffers.to(device), False, h, c)\n",
    "        h.to(device)\n",
    "        c.to(device)\n",
    "\n",
    "        if any(hit_fail_mask):\n",
    "            losses = loss(out, optimal_predictions.to(device))\n",
    "            masked_losses = losses[hit_fail_mask]\n",
    "            loss_value = masked_losses.mean()\n",
    "    \n",
    "            loss_value.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss_value.item()\n",
    "            loss_avg = loss_sum / (batch_end // BATCH_SIZE)\n",
    "\n",
    "            pbar.set_postfix_str(f\"loss={loss_avg}\")\n",
    "\n",
    "        # f.write(\"=========================\\n\")\n",
    "        # for i in range(len(hit_fail_mask)):\n",
    "        #     if hit_fail_mask[i]:\n",
    "        #         f.write(f\"{i}. ////\\n\")\n",
    "        #         f.write(f\"{out[i]}\\n\")\n",
    "        #         f.write(f\"{optimal_predictions[i]}\\n\")\n",
    "\n",
    "        #         for name, param in model.named_parameters():\n",
    "        #             if param.grad is not None:\n",
    "        #                 f.write(f\"{name}: {param.grad.abs().mean()}\\n\")\n",
    "\n",
    "        #         f.write(\"////\\n\")\n",
    "        # f.write(\"=========================\\n\")\n",
    "\n",
    "        h = h.detach()\n",
    "        c = c.detach()\n",
    "\n",
    "    torch.save(model.state_dict(), \"drive/MyDrive/model1.pth\")\n",
    "    torch.save(optimizer.state_dict(), \"drive/MyDrive/opt1.pth\")\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
